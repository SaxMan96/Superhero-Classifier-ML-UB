{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and select the kaggle.json file downloaded\n",
    "# from the Kaggle account settings page.\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure the kaggle.json file is present.\n",
    "!ls -lha kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, install the Kaggle API client.\n",
    "!pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
    "# so move it there.\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This permissions change avoids a warning on Kaggle tool startup.\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available datasets.\n",
    "!kaggle competitions download -c superhero-or-supervillain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, make_scorer, f1_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "# Load Data\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.metrics import classification_report, , f1_score, accuracy_score, roc_curve, auc\n",
    "\n",
    "\n",
    "def load_data() -> pd.Series:\n",
    "    csv_train = pd.read_csv('/content/train.csv').assign(train = 1) \n",
    "    csv_test = pd.read_csv('/content/test.csv').assign(train = 0) \n",
    "    csv = pd.concat([csv_train,csv_test], sort=True)\n",
    "    return csv\n",
    "\n",
    "# Analyze Data\n",
    "\n",
    "def nans_ctr(csv) -> pd.Series:\n",
    "    return csv.isna().sum()\n",
    "\n",
    "def unique_ctr(csv) -> pd.Series():\n",
    "    unique = pd.Series()\n",
    "    for col in list(csv):\n",
    "        if(csv.columns.contains(col)):\n",
    "            unique.at[col] = len(csv[col].unique())\n",
    "    return unique\n",
    "\n",
    "def val_types(csv) -> pd.Series():\n",
    "    val_type = pd.Series()\n",
    "    for col in list(csv):\n",
    "        if not csv.columns.contains(col):\n",
    "            continue\n",
    "        if csv[col].dtype == np.float64:\n",
    "            val_type.at[col] = np.float64\n",
    "        elif csv[col].dtype == np.int64:\n",
    "            val_type.at[col] = np.int64\n",
    "        elif csv[col].dtype == np.int32:\n",
    "            val_type.at[col] = np.int32\n",
    "        elif csv[col].dtype == np.uint8:\n",
    "            val_type.at[col] = np.uint8\n",
    "        elif csv[col].dtype == object:\n",
    "            val_type.at[col] = object\n",
    "        elif csv[col].dtype == bool:\n",
    "            val_type.at[col] = bool\n",
    "        else:\n",
    "            print(f\"No common value type found in val_types() - {csv[col].dtype}\")\n",
    "    return val_type\n",
    "\n",
    "def min_max_val(csv) -> pd.Series():\n",
    "    min_val = pd.Series()\n",
    "    max_val = pd.Series()\n",
    "    val_type = val_types(csv)\n",
    "    for col in list(csv):\n",
    "        if val_type[col] != object:\n",
    "            min_val.at[col] = csv[col].min()\n",
    "            max_val.at[col] = csv[col].max()\n",
    "        else:    \n",
    "            min_val.at[col] = None\n",
    "            max_val.at[col] = None\n",
    "    return min_val, max_val        \n",
    "            \n",
    "def get_stats(csv):\n",
    "    nans = nans_ctr(csv)\n",
    "    unique = unique_ctr(csv)\n",
    "    val_type = val_types(csv)\n",
    "    min_val, max_val = min_max_val(csv)\n",
    "    result = pd.DataFrame({ 'nans': nans, 'unique': unique, 'val_type': val_type, 'min_val': min_val, 'max_val': max_val}) \n",
    "    return result\n",
    "    \n",
    "def bool_to_integer(csv) -> pd.DataFrame():\n",
    "    for col in csv.columns:\n",
    "        if csv[col].dtype == bool:\n",
    "            csv[col] = csv[col].astype(int)\n",
    "    return csv\n",
    "    \n",
    "def standarize_numerical_values(csv):\n",
    "    for col in csv.columns:\n",
    "        if col == 'train':\n",
    "            continue\n",
    "        if csv[col].dtype == np.float64:\n",
    "            data = csv[col]\n",
    "            std = data.std()\n",
    "            data = data[(data < data.quantile(0.99)) & (data > data.quantile(0.01))]\n",
    "            mean = data.mean()\n",
    "            csv[col] = (csv[col] - mean)/std\n",
    "#             _ = plt.hist(csv[col], bins='auto', alpha = 0.5)\n",
    "#             plt.yscale('log')\n",
    "#             plt.title(f\"Distr in {col} column\")\n",
    "#             plt.show()\n",
    "    return csv\n",
    "\n",
    "def check_rows(csv):\n",
    "    for row in range(len(csv)):\n",
    "        print(row, csv.iloc[row].isna().sum())\n",
    "    return csv\n",
    "\n",
    "def distribution_in_columns(csv):\n",
    "    for col in list(csv):\n",
    "        print(csv[col].value_counts())\n",
    "    return csv\n",
    "        \n",
    "def plot_dist_y(csv):\n",
    "    plt.pie([len(csv[csv['Alignment'] == 'good']), len(csv[csv['Alignment'] == 'bad']), \n",
    "             len(csv[csv['Alignment'] == 'neutral'])], labels = ['good', 'bad', 'neutral'])\n",
    "    plt.show()\n",
    "    return csv\n",
    "    \n",
    "def factorize(csv) -> pd.DataFrame():\n",
    "    for col in csv.select_dtypes(include=['object']).columns:\n",
    "        if col == \"Alignment\":\n",
    "            continue\n",
    "        dummy = pd.get_dummies(csv[col])\n",
    "        dummy.columns = [col+ \" \"+x for x in dummy.columns]\n",
    "        dummy = dummy.drop([dummy.columns[-1]], axis=1)\n",
    "        csv = csv.drop(col, axis=1)\n",
    "        csv = pd.concat([csv, dummy], axis=1)\n",
    "    return csv\n",
    "    \n",
    "def prepare_models(use_scaler = False, use_grid_search = False, verbose=False):\n",
    "    gs_dict = {}\n",
    "    clfs_dict = {\n",
    "#         'DTC': DecisionTreeClassifier(random_state=0),\n",
    "#         'RFC': RandomForestClassifier(random_state=0),\n",
    "#         'KNN': KNeighborsClassifier(),\n",
    "#         'LR': LogisticRegression(random_state=35)\n",
    "#         'XGB': XGBClassifier(random_state=0)\n",
    "        'SVC': SVC(random_state=0)\n",
    "    }\n",
    "    param_grids = {\n",
    "        'DTC': {\n",
    "            '__max_depth': [None,1,2,3,4,5,7,10,15],\n",
    "            '__criterion': ['gini', 'entropy'],\n",
    "            '__class_weight': [None],\n",
    "            '__presort': [False,True],\n",
    "#             '__min_samples_split': np.linspace(0.001,1.0,7,endpoint=True),\n",
    "#             '__min_weight_fraction_leaf': np.linspace(0.05,0.4,5) \n",
    "        },\n",
    "        'RFC': { \n",
    "            '__n_estimators': [15,50, 100],\n",
    "            '__max_depth' : [1,2,3,5,10,20],\n",
    "            '__criterion' : ['gini', 'entropy'],\n",
    "            '__class_weight': [None,'balanced'],\n",
    "            '__max_features' : list(range(6,32,5))+['auto']\n",
    "        },\n",
    "        'KNN': {\n",
    "            '__n_neighbors': range(1, 12),\n",
    "            '__weights': ['uniform', 'distance'],\n",
    "        },\n",
    "        'LR': {\n",
    "            \"__C\": np.logspace(-4,4,20), \n",
    "            \"__penalty\":[\"l1\",\"l2\"]\n",
    "        },\n",
    "        'XGB': {\n",
    "            '__n_estimators': [80, 100,200,300]\n",
    "            '__min_child_weight': [1, 5, 10],\n",
    "            '__gamma': [0.5, 1, 1.5, 2, 5],\n",
    "            '__subsample': [0.6, 0.8, 1.0],\n",
    "            '__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            '__max_depth': [3, 4, 5]\n",
    "        },\n",
    "        \n",
    "        \n",
    "        \n",
    "        max_depth: The depth of each estimator tree. Typical values are 3 to 10.\n",
    "        subsample: The percentage of samples that are used to build each estimator tree. Typical value range is 0.8 to 1.0.\n",
    "        colsample_bytree: The percentage of columns used to build each tree. The range depends on the number of columns / features in the dataset. Should be 1.0 for datasets with few columns.\n",
    "        gamma: A regularization parameter. Usually ranging from 0 to 5. If you notice massive overfitting in the data (training set performance much better than test set performance) try bigger values.\n",
    "        \n",
    "        'SVC': {\n",
    "            '__kernel':('linear', 'rbf'), \n",
    "            '__C':(1,0.25,0.5,0.75),\n",
    "            '__gamma': (1,2,3,5,10,'auto'),\n",
    "            '__decision_function_shape':('ovo','ovr'),\n",
    "            '__shrinking':(True,False)\n",
    "        }\n",
    "    }\n",
    "    scorer = make_scorer(f1_score, average='micro')            \n",
    "    for clf_name in clfs_dict:\n",
    "        if use_scaler:\n",
    "            pipe_line = Pipeline([('scaler', StandardScaler()), ('', clfs_dict[clf_name])])\n",
    "        else:\n",
    "            pipe_line = clfs_dict[clf_name]\n",
    "            \n",
    "        if use_grid_search:\n",
    "            gs = GridSearchCV(pipe_line, param_grid=param_grids[clf_name], scoring=scorer, verbose=verbose)\n",
    "        else:\n",
    "            gs = pipe_line\n",
    "        gs_dict[clf_name] = gs\n",
    "    return gs_dict\n",
    "        \n",
    "def model_selection(x_train, x_valid, y_train, y_valid):\n",
    "    gs_dict = prepare_models(use_scaler = True, use_grid_search = True, verbose=False)\n",
    "#     gs_dict = prepare_models(verbose=True)\n",
    "    for gs_name in gs_dict:        \n",
    "        print(\"\\n==================================================================\\n\", gs_name, end=\"\\t\")\n",
    "        gs = gs_dict[gs_name].fit(x_train,y_train)\n",
    "        print(\"\\tAccuracy Train: \", np.round(100*accuracy_score(y_train,gs.predict(x_train)),1),\"%\",sep=\"\", end=\"\")\n",
    "        print(\"\\tAccuracy Valid: \", np.round(100*accuracy_score(y_valid,gs.predict(x_valid)),1),\"%\",sep=\"\")\n",
    "        print(\"\\tBest params: \", gs.best_params_)\n",
    "        print(pd.DataFrame(confusion_matrix(y_valid, gs.predict(x_valid)),\n",
    "                     columns=['Predicted 0', 'Predicted 1', 'Predicted 2'],\n",
    "                     index=['Actual 0', 'Actual 1', 'Actual 2']))\n",
    "        gs_dict[gs_name] = gs\n",
    "    return gs_dict\n",
    "\n",
    "def predict(clf, x_test):\n",
    "    x_test = pd.DataFrame(x_test).fillna(0)\n",
    "    y_predict = clf.predict(x_test)\n",
    "    y_predict = pd.DataFrame(y_predict)\n",
    "    y_predict[y_predict == 0] = \"bad\"\n",
    "    y_predict[y_predict == 1] = \"good\"\n",
    "    y_predict[y_predict == 2] = \"neutral\"\n",
    "    y_predict.columns = [\"Prediction\"]\n",
    "    y_predict.to_csv('result.csv',index_label =\"Id\")\n",
    "\n",
    "\n",
    "def pca_decomp(x_tr, x_valid, y_tr, y_valid):\n",
    "    x_tr = StandardScaler().fit_transform(x_tr)\n",
    "    pca = PCA(n_components=0.9, svd_solver='full', random_state=0).fit(x_tr)\n",
    "    x_tr = pca.transform(x_tr)\n",
    "    x_valid = pca.transform(x_valid)\n",
    "    print(len(pca.components_))\n",
    "    return x_tr, x_valid, y_tr, y_valid\n",
    "\n",
    "def feature_extraction(csv):\n",
    "    csv = csv.drop(columns=['Id'])\n",
    "    csv = standarize_numerical_values(csv)\n",
    "    csv = bool_to_integer(csv)\n",
    "    csv = factorize(csv)\n",
    "    stats = get_stats(csv)\n",
    "    # with pd.option_context('display.max_rows', 500, 'display.max_columns', 500, 'display.width', 1000):\n",
    "        # print(stats)\n",
    "    Y = csv[\"Alignment\"]\n",
    "    csv = csv.drop(\"Alignment\",axis=1)\n",
    "    X = csv\n",
    "    is_train = csv[\"train\"] == 1\n",
    "    y_train, uniques = pd.factorize(Y[is_train])\n",
    "    x_train = X[is_train].values\n",
    "    x_train = pd.DataFrame(x_train).fillna(0)\n",
    "    x_test = X[is_train == False].values\n",
    "    return x_train, y_train, x_test\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    csv = load_data()\n",
    "    x_train, y_train, x_test = feature_extraction(csv)\n",
    "    x_tr, x_valid, y_tr, y_valid = train_test_split(x_train, y_train, test_size=0.3,random_state=0)\n",
    "    \n",
    "#     x_tr, x_valid, y_tr, y_valid = pca_decomp(x_tr, x_valid, y_tr, y_valid)     \n",
    "#     for i in [x_tr, y_tr, x_valid, y_valid]:\n",
    "#         print(i.shape)\n",
    "#     print('Original dataset shape train: %s' % Counter(y_tr))\n",
    "    sm = SMOTE(random_state=0)\n",
    "    x_tr, y_tr = sm.fit_resample(x_tr, y_tr)\n",
    "    print('Original dataset shape train: %s' % Counter(y_tr))\n",
    "    \n",
    "    gs_dict = model_selection(x_tr,x_valid, y_tr, y_valid)\n",
    "    \n",
    "#     clf = gs_dict[\"LR\"]\n",
    "#     predict(clf, x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
